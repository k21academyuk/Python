{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### What is PySpark?\n",
        "\n",
        "PySpark is the Python API for **Apache Spark**, a powerful open-source framework for **distributed computing**. It allows for large-scale data processing, providing easy-to-use APIs for working with big data and performing **data analysis**. Spark can process data much faster than traditional tools, such as **Hadoop**, by keeping data in memory rather than writing intermediate results to disk.\n",
        "\n",
        "Spark works by distributing the data processing tasks across multiple nodes (machines) in a cluster, allowing it to handle massive amounts of data in parallel. PySpark brings the distributed processing capabilities of Apache Spark to Python, making it accessible for data scientists, engineers, and analysts familiar with Python.\n",
        "\n",
        "### Key Concepts in PySpark\n",
        "\n",
        "1. **Resilient Distributed Datasets (RDDs)**:\n",
        "   - RDD is the fundamental data structure in Spark. It represents a collection of objects that can be processed in parallel across multiple nodes in a cluster.\n",
        "   - RDDs are **immutable**, meaning once they are created, they cannot be modified. However, transformations can be applied to generate new RDDs.\n",
        "\n",
        "2. **SparkContext**:\n",
        "   - The **SparkContext** is the entry point to any Spark functionality. It establishes the connection to the Spark cluster, enabling the user to create RDDs, broadcast variables, and perform actions on them.\n",
        "\n",
        "3. **SparkSession**:\n",
        "   - The **SparkSession** is the newer and recommended entry point for working with PySpark, as it provides a more unified interface. It allows working with both **RDDs** and **DataFrames**.\n",
        "   - SparkSession is used to create and manage Spark jobs and enables working with SQL, DataFrame, and Dataset API.\n",
        "\n",
        "4. **Transformations vs Actions**:\n",
        "   - **Transformations** are operations that define a new RDD or DataFrame from an existing one, like **map()**, **filter()**, **flatMap()**. Transformations are **lazy**, meaning they are only executed when an **action** is triggered.\n",
        "   - **Actions** are operations that return a result to the driver or store data to an external storage system, like **count()**, **collect()**, **save()**.\n",
        "\n",
        "5. **DataFrames**:\n",
        "   - DataFrames are similar to **tables** in relational databases and **Pandas DataFrames** in Python. They provide a higher-level abstraction than RDDs and are optimized for performing **complex queries** and **data manipulations**.\n",
        "\n",
        "### Why Use PySpark?\n",
        "\n",
        "- **Scalability**: PySpark can scale up from a single machine to thousands of nodes, processing petabytes of data.\n",
        "- **Speed**: PySpark can process data in-memory, which significantly improves the speed of computation compared to traditional systems like Hadoop MapReduce.\n",
        "- **Ease of Use**: PySpark provides a high-level API for distributed data processing that is accessible to Python developers, making it easier to integrate into Python-based workflows.\n",
        "- **Unified Processing**: PySpark supports both batch processing and stream processing, giving you flexibility for various use cases.\n",
        "\n",
        "### PySpark Ecosystem\n",
        "\n",
        "1. **Spark SQL**: Allows querying data via SQL syntax.\n",
        "2. **MLlib**: Spark’s scalable machine learning library, providing algorithms for classification, regression, clustering, and more.\n",
        "3. **GraphX**: Spark's library for graph processing.\n",
        "4. **Spark Streaming**: Enables real-time stream processing with Spark.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vjRxg3QriMG3"
      },
      "id": "vjRxg3QriMG3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Big Data\n",
        "\n",
        "**Big Data** refers to a vast amount of data that is difficult to process using traditional data processing tools due to its **large volume**, **complexity**, and **variety**. It includes data that is so massive in size and diverse in format that traditional databases and processing systems struggle to handle it efficiently.\n",
        "\n",
        "### Key Characteristics of Big Data:\n",
        "- **Volume**: The sheer amount of data generated daily is enormous.\n",
        "- **Variety**: Big data can come in different formats such as structured, unstructured, and semi-structured.\n",
        "- **Velocity**: Data is generated at high speeds, requiring real-time processing.\n",
        "- **Veracity**: The uncertainty of data due to its vastness and inconsistency.\n",
        "\n",
        "---\n",
        "\n",
        "# Big Data in Social Media\n",
        "\n",
        "Did you know that **500+ terabytes** of new data are ingested into Facebook's databases every day? This data includes:\n",
        "- Photo and video uploads\n",
        "- Message exchanges\n",
        "- Comments and other interactions\n",
        "\n",
        "Additionally, industries like aviation generate massive amounts of data. For example:\n",
        "- A single **jet engine** can produce **10+ terabytes of data** in just **30 minutes** of flight time.\n",
        "- With **thousands of flights per day**, the total data generated amounts to **petabytes**.\n",
        "\n",
        "---\n",
        "\n",
        "# Types of Big Data\n",
        "\n",
        "Big Data can be categorized into three types:\n",
        "\n",
        "### 1. Structured Data\n",
        "- Data that is organized in a fixed format, typically in rows and columns (e.g., databases).\n",
        "- Easily stored, accessed, and processed using traditional data tools (like SQL).\n",
        "\n",
        "### 2. Unstructured Data\n",
        "- Data that has no predefined structure or format. It can come in various forms, including:\n",
        "  - Text files\n",
        "  - Images\n",
        "  - Videos\n",
        "- This type of data is harder to process but provides rich insights when analyzed.\n",
        "\n",
        "### 3. Semi-structured Data\n",
        "- Data that falls between structured and unstructured, containing elements of both.\n",
        "- Example: JSON files, XML, and NoSQL databases that have flexible schemas.\n",
        "\n",
        "---\n",
        "\n",
        "# Big Data Tools\n",
        "\n",
        "To handle Big Data, several tools are commonly used:\n",
        "\n",
        "- **Apache Hadoop**: A framework for distributed storage and processing of large datasets.\n",
        "- **Apache Spark**: A fast and general-purpose cluster-computing system for large-scale data processing.\n",
        "- **Apache Cassandra**: A highly scalable NoSQL database designed for handling large amounts of data across many commodity servers.\n",
        "- **NoSQL Databases**: Databases like MongoDB and Couchbase that handle unstructured data.\n",
        "- **Data Visualization Tools**: Tools like Tableau and Power BI for visualizing complex data insights.\n",
        "- **Machine Learning Libraries**: Libraries such as TensorFlow and Scikit-learn for applying machine learning to big data.\n",
        "\n",
        "---\n",
        "\n",
        "# What is Apache Spark?\n",
        "\n",
        "**Apache Spark** is a powerful, **lightning-fast** cluster-computing technology designed for big data processing. It extends the traditional **Hadoop MapReduce** model and includes support for:\n",
        "- **Interactive Queries**: Allowing real-time data analysis.\n",
        "- **Stream Processing**: Real-time data processing capabilities.\n",
        "\n",
        "Spark can handle a variety of workloads, including:\n",
        "- **Batch Applications**: Processing large volumes of data at once.\n",
        "- **Iterative Algorithms**: Running algorithms that require multiple passes over the data.\n",
        "- **Interactive Queries**: Performing quick queries on massive datasets.\n",
        "- **Streaming Data**: Handling real-time data processing needs.\n",
        "\n",
        "---\n",
        "\n",
        "# Evolution of Apache Spark\n",
        "\n",
        "- **2009**: Apache Spark was developed as a subproject of Hadoop.\n",
        "- **2010**: It was open-sourced under a BSD license and became an independent project.\n",
        "- Since then, Spark has grown in popularity due to its speed and versatility in handling Big Data workloads.\n",
        "\n",
        "---\n",
        "\n",
        "# Features of Apache Spark\n",
        "\n",
        "1. **Speed**: Spark runs applications on a Hadoop cluster up to **100 times faster** than traditional MapReduce.\n",
        "2. **Supports Multiple Languages**: Spark provides built-in APIs in:\n",
        "   - **Java**\n",
        "   - **Scala**\n",
        "   - **Python**\n",
        "3. **Advanced Analytics**: Spark supports a variety of advanced analytics, including:\n",
        "   - **SQL Queries**\n",
        "   - **Stream Processing**\n",
        "   - **Machine Learning (ML)**\n",
        "   - **Graph Algorithms**\n",
        "\n",
        "---\n",
        "\n",
        "# Spark’s Basic Architecture\n",
        "\n",
        "Spark runs on a **cluster** of machines that work together to process data. The cluster is managed by a **Cluster Manager**, which can be:\n",
        "- **Spark's standalone cluster manager**\n",
        "- **YARN** (Yet Another Resource Negotiator)\n",
        "- **Mesos**\n",
        "\n",
        "The **SparkContext** is responsible for managing the cluster and initializing the Spark application.\n",
        "\n",
        "---\n",
        "\n",
        "# Spark’s Language APIs\n",
        "\n",
        "Spark provides APIs in several programming languages, allowing developers to use the language they are most comfortable with:\n",
        "- **Scala**\n",
        "- **Java**\n",
        "- **Python**\n",
        "- **SQL**\n",
        "- **R**\n",
        "\n",
        "---\n",
        "\n",
        "# Spark’s APIs\n",
        "\n",
        "Spark offers two main types of APIs:\n",
        "1. **Low-level unstructured APIs**: For advanced users who need full control over Spark's operations.\n",
        "2. **High-level structured APIs**: Easier to use and optimized for most use cases, focusing on DataFrames and SQL.\n",
        "\n",
        "**PySpark** is the Python API for Apache Spark. It allows Python developers to interact with Spark through the Python programming language, making it accessible to data scientists and engineers.\n",
        "\n",
        "---\n",
        "\n",
        "# PySpark Abstractions: RDDs and DataFrames\n",
        "\n",
        "- **RDDs (Resilient Distributed Datasets)**: The fundamental data structure in Spark. RDDs are immutable and support fault-tolerant parallel processing across multiple machines.\n",
        "- **DataFrames**: A higher-level abstraction over RDDs that resembles tables in relational databases. DataFrames are optimized for querying and transforming data.\n"
      ],
      "metadata": {
        "id": "0KCD-Pgyi4BW"
      },
      "id": "0KCD-Pgyi4BW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up PySpark\n",
        "\n",
        "Before we start working with PySpark, you need to install **PySpark** in your Python environment. To install it, you can use the following command in a Jupyter notebook cell:\n",
        "\n"
      ],
      "metadata": {
        "id": "37RtwJSTj9Py"
      },
      "id": "37RtwJSTj9Py"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abcaad4",
      "metadata": {
        "id": "4abcaad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc8e25c-a744-47ee-b940-205ac37aab67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing PySpark"
      ],
      "metadata": {
        "id": "5FkvAe7WkJqt"
      },
      "id": "5FkvAe7WkJqt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b29f6a",
      "metadata": {
        "id": "08b29f6a"
      },
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up SparkContext\n",
        "\n",
        " **SparkConf** allows you to set various configuration parameters for a Spark application.\n",
        "- You can use SparkConf to configure settings such as the application name, Spark master URL, and other runtime properties.\n",
        "\n",
        "**SparkContext** It is responsible for coordinating the execution of Spark jobs on a cluster"
      ],
      "metadata": {
        "id": "vNoWuJClObSC"
      },
      "id": "vNoWuJClObSC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**conf = SparkConf().setMaster(\"local\").setAppName(\"HappyLearning\")**\n",
        "\n",
        "---\n",
        "\n",
        "creates a Spark configuration object (conf) that tells Apache Spark how to run your application.\n",
        "- .setMaster(\"local\") - tells Spark where to run. \"local\" means run on your local machine, using a single thread.\n",
        "\n",
        "- .setAppName(\"HappyLearning\") - sets the name of your Spark application."
      ],
      "metadata": {
        "id": "hC0c6pHfpBwY"
      },
      "id": "hC0c6pHfpBwY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X9UMgikxrUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "9e394b0d-7264-4a67-a909-a89af5be27d7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local appName=HappyLearning>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f671d307e5d5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>HappyLearning</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"HappyLearning\")\n",
        "sc = SparkContext(conf = conf)\n",
        "sc"
      ],
      "id": "1X9UMgikxrUd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5dlFBHrxrUf"
      },
      "source": [
        "### Creating an RDD (Resilient Distributed Dataset)"
      ],
      "id": "W5dlFBHrxrUf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBfR_tDExrUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada0da9a-5a95-46bd-d384-48262242bb5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "stringRDD = sc.parallelize([\"Spark is awesome\",\"Spark is cool\"])\n",
        "stringRDD"
      ],
      "id": "SBfR_tDExrUg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the RDD"
      ],
      "metadata": {
        "id": "YVAMevHMkefs"
      },
      "id": "YVAMevHMkefs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FAqI-cBxrUg",
        "outputId": "ca50049f-68ef-4a79-a451-07cf9355b5ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "stringRDD"
      ],
      "id": "9FAqI-cBxrUg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collecting the RDD Data"
      ],
      "metadata": {
        "id": "_xWBBN40kieC"
      },
      "id": "_xWBBN40kieC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93cNNR2cxrUg",
        "outputId": "8c45f260-6349-474b-cec3-b45fdf2c05d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark is awesome', 'Spark is cool']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "stringRDD.collect()"
      ],
      "id": "93cNNR2cxrUg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr00V8gexrUl"
      },
      "source": [
        "### Transforming Data in RDDs (Using `map()`)"
      ],
      "id": "Sr00V8gexrUl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBJxiywxxrUm",
        "outputId": "5b46af2f-d35a-4c1a-b722-6f941c7a8a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SPARK IS AWESOME', 'SPARK IS COOL']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "stringRDD_uppercase= stringRDD.map(lambda x: x.upper())\n",
        "stringRDD_uppercase.collect()"
      ],
      "id": "jBJxiywxxrUm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWJZKNSwxrUm"
      },
      "source": [
        "### Using `flatMap()` Transformation"
      ],
      "id": "UWJZKNSwxrUm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-0_nYE4xrUm",
        "outputId": "b9ab1ad1-e916-4634-9b8b-3fa7a077719d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark', 'is', 'awesome', 'Spark', 'is', 'cool']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "flatMap_Split= stringRDD.flatMap(lambda x: x.split(\" \"))\n",
        "flatMap_Split.collect()"
      ],
      "id": "0-0_nYE4xrUm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZToPDsqxrUp"
      },
      "source": [
        "### Collecting Data from RDD\n",
        "\n"
      ],
      "id": "5ZToPDsqxrUp"
    },
    {
      "cell_type": "code",
      "source": [
        "stringRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHObIAOrkIQh",
        "outputId": "a9e821bf-9b55-492b-8480-da8770585cdf"
      },
      "id": "lHObIAOrkIQh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark is awesome', 'Spark is cool']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `filter()` Transformation"
      ],
      "metadata": {
        "id": "KFVj65DPAbzR"
      },
      "id": "KFVj65DPAbzR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdXJ0hsMxrUp",
        "outputId": "759d3288-316f-464b-cc73-fd14383999a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark is awesome']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "awesomeLineRDD = stringRDD.filter(lambda x: \"awesome\" in x)\n",
        "awesomeLineRDD.collect()"
      ],
      "id": "FdXJ0hsMxrUp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `filter()` with Case Insensitivity"
      ],
      "metadata": {
        "id": "gyqZIazvAkdS"
      },
      "id": "gyqZIazvAkdS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3lpGXomxrUv",
        "outputId": "037a5671-fbb3-4931-e9e0-366d1006bddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark is awesome', 'Spark is cool']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sparkLineRDD = stringRDD.filter(lambda x: \"spark\" in x.lower())\n",
        "sparkLineRDD.collect()"
      ],
      "id": "s3lpGXomxrUv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `pandas` to Read a CSV File"
      ],
      "metadata": {
        "id": "pUPBX69UApX5"
      },
      "id": "pUPBX69UApX5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1de79e4",
      "metadata": {
        "id": "e1de79e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "a3b5d2e3-8c20-4760-d114-54881d2b40d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 509);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import pandas as pd\n",
        "type(pd.read_csv('test1.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is SparkSession in PySpark?\n",
        "\n",
        "In **PySpark**, **SparkSession** is the main entry point to interact with **Apache Spark**. It serves as the interface for working with structured data (e.g., DataFrames) and running SQL queries. A **SparkSession** allows you to manage the Spark application and access the various Spark features.\n",
        "\n",
        "#### Key Features of SparkSession:\n",
        "- **Create DataFrames**: You can create **DataFrames** using data from various sources (CSV, Parquet, JSON, etc.) and perform data transformations on them.\n",
        "- **Register DataFrames as Tables**: SparkSession allows you to register DataFrames as temporary SQL tables, enabling SQL queries on the DataFrame.\n",
        "- **Execute SQL Queries**: With SparkSession, you can execute **SQL queries** on registered tables, enabling you to perform SQL-based operations on your data.\n",
        "- **Distributed Data Operations**: It also enables performing distributed data processing using Spark’s **engine**, allowing operations to be applied on large datasets spread across multiple nodes.\n",
        "\n",
        "#### Example Usage:\n",
        "1. **Creating DataFrames**: You can read data from external sources (e.g., CSV, JSON, or Parquet) and create DataFrames.\n",
        "2. **SQL Queries**: After registering a DataFrame as a table, you can run SQL queries directly using **`spark.sql()`**.\n",
        "3. **Optimized Execution**: SparkSession provides an optimized engine for processing large-scale data, including lazy evaluation and distributed computing.\n",
        "\n",
        "### Key Points:\n",
        "- **SparkSession** is the main entry point for using **Spark SQL** and working with **structured data** in PySpark.\n",
        "- It simplifies the process of managing Spark sessions, enabling the use of both **DataFrame** and **SQL** APIs.\n"
      ],
      "metadata": {
        "id": "CpRTa0lH5NHV"
      },
      "id": "CpRTa0lH5NHV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing SparkSession\n"
      ],
      "metadata": {
        "id": "JHp4TSFSAyam"
      },
      "id": "JHp4TSFSAyam"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c334b45e",
      "metadata": {
        "id": "c334b45e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Practise').getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558caca5",
      "metadata": {
        "id": "558caca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "a5574ab0-6381-4792-a505-6e983934cd41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a7028ff53d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f671d307e5d5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>HappyLearning</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Data from a CSV File and Displaying It"
      ],
      "metadata": {
        "id": "NA1-dQxcBBY0"
      },
      "id": "NA1-dQxcBBY0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ac726b",
      "metadata": {
        "id": "f7ac726b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb53f75-d503-4e29-a46a-153f86bdebc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|    _c0|_c1|       _c2|   _c3|\n",
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark = spark.read.csv('test1.csv')\n",
        "df_pyspark\n",
        "\n",
        "df_pyspark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading CSV with Header and Displaying Data"
      ],
      "metadata": {
        "id": "j0TQd3SJBF1Q"
      },
      "id": "j0TQd3SJBF1Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f077d49",
      "metadata": {
        "id": "6f077d49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd426196-a35c-40a8-a564-23166cc1818f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[Name: string, age: string, Experience: string, Salary: string]\n",
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark = spark.read.option('header','true').csv('test1.csv')\n",
        "print(df_pyspark)  # Check Schema\n",
        "df_pyspark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the DataFrame Type"
      ],
      "metadata": {
        "id": "AYd-UDeWBJta"
      },
      "id": "AYd-UDeWBJta"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0eee51",
      "metadata": {
        "id": "2e0eee51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "f78a223f-62b8-4a50-c069-ef343387de72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "type(df_pyspark)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Schema of a DataFrame"
      ],
      "metadata": {
        "id": "R2Te0u6GBPGM"
      },
      "id": "R2Te0u6GBPGM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb26bf24",
      "metadata": {
        "id": "bb26bf24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdf5766-804e-404d-b20a-f1fd0c5d22bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Top Rows of a DataFrame"
      ],
      "metadata": {
        "id": "8V0KB_C1BUsB"
      },
      "id": "8V0KB_C1BUsB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a81cc1",
      "metadata": {
        "id": "17a81cc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73805a65-91df-4b17-8ba9-c2568e5f07c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|    _c0|_c1|       _c2|   _c3|\n",
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying DataFrame Column Data Types"
      ],
      "metadata": {
        "id": "tQf05H1oBZkK"
      },
      "id": "tQf05H1oBZkK"
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_pyspark.dtypes)\n"
      ],
      "metadata": {
        "id": "_gED_YbR8WDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3688269b-4720-4a9b-f87e-57f7558d783a"
      },
      "id": "_gED_YbR8WDD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]\n",
            "DataFrame[Name: string, age: int, Experience: int, Salary: int]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Column Names of a DataFrame"
      ],
      "metadata": {
        "id": "79s0vipBBeGz"
      },
      "id": "79s0vipBBeGz"
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_pyspark.columns)"
      ],
      "metadata": {
        "id": "X5JTl3a-7c_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ea1348-184c-4ab3-9667-ab51227c190f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Name', 'age', 'Experience', 'Salary']\n"
          ]
        }
      ],
      "id": "X5JTl3a-7c_M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the First Few Rows of a DataFrame"
      ],
      "metadata": {
        "id": "Ps13u9eCBk8l"
      },
      "id": "Ps13u9eCBk8l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4a3c8eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea093d20-db20-449e-f1af-2d5fe3a59183"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Name='Sourabh', age=31, Experience=10, Salary=30000),\n",
              " Row(Name='Disha', age=30, Experience=8, Salary=25000),\n",
              " Row(Name='Sunny', age=29, Experience=4, Salary=20000)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df_pyspark.head(3)"
      ],
      "id": "e4a3c8eb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the First Few Rows of a DataFrame"
      ],
      "metadata": {
        "id": "WZSF5lrqBo7J"
      },
      "id": "WZSF5lrqBo7J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5523ae24",
        "outputId": "a73bc5d2-ba01-4c57-9c2a-d53932d28bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.show()"
      ],
      "id": "5523ae24"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting Specific Columns from a DataFrame"
      ],
      "metadata": {
        "id": "Hgc-Kffr8Igp"
      },
      "id": "Hgc-Kffr8Igp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c513816d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f86bbd-fa9e-406c-9033-fe794604379c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|   Name|Experience|\n",
            "+-------+----------+\n",
            "|Sourabh|        10|\n",
            "|  Disha|         8|\n",
            "|  Sunny|         4|\n",
            "|   Paul|         3|\n",
            "| Harsha|         1|\n",
            "|Shubham|         2|\n",
            "+-------+----------+\n",
            "\n",
            "+-------+\n",
            "|   Name|\n",
            "+-------+\n",
            "|Sourabh|\n",
            "|  Disha|\n",
            "|  Sunny|\n",
            "|   Paul|\n",
            "| Harsha|\n",
            "|Shubham|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.select(['Name','Experience']).show()\n",
        "\n",
        "df_pyspark.select('Name').show()"
      ],
      "id": "c513816d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Summary Statistics for a DataFrame\n"
      ],
      "metadata": {
        "id": "xVJc4bj0B0Vw"
      },
      "id": "xVJc4bj0B0Vw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa74b18e"
      },
      "outputs": [],
      "source": [
        "df_pyspark.describe().show()"
      ],
      "id": "fa74b18e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a New Column to a DataFrame"
      ],
      "metadata": {
        "id": "w7DnB7dZ9O2O"
      },
      "id": "w7DnB7dZ9O2O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a8d7b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf8fc9a-18fb-421d-e476-c84ac369dc0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Name: string, age: string, Experience: string, Salary: string, Experience After 2 year: double]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df_pyspark=df_pyspark.withColumn('Experience After 2 year',df_pyspark['Experience']+2)\n",
        "df_pyspark\n"
      ],
      "id": "6a8d7b54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Updated DataFrame"
      ],
      "metadata": {
        "id": "hLFx9UJPCFsn"
      },
      "id": "hLFx9UJPCFsn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9ff01ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13ddbb2-4d7a-4805-c04f-febcdf960cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+-----------------------+\n",
            "|   Name|age|Experience|Salary|Experience After 2 year|\n",
            "+-------+---+----------+------+-----------------------+\n",
            "|Sourabh| 31|        10| 30000|                   12.0|\n",
            "|  Disha| 30|         8| 25000|                   10.0|\n",
            "|  Sunny| 29|         4| 20000|                    6.0|\n",
            "|   Paul| 24|         3| 20000|                    5.0|\n",
            "| Harsha| 21|         1| 15000|                    3.0|\n",
            "|Shubham| 23|         2| 18000|                    4.0|\n",
            "+-------+---+----------+------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.show()"
      ],
      "id": "e9ff01ed"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping a Column from a DataFrame: **drop()**"
      ],
      "metadata": {
        "id": "RRQHQkVDK8R_"
      },
      "id": "RRQHQkVDK8R_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d98641db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64366d45-10b2-43ee-bd9c-59fa6f81bcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark=df_pyspark.drop('Experience After 2 year')\n",
        "df_pyspark.show()\n"
      ],
      "id": "d98641db"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Renaming a Column in a DataFrame - **withColumnRenamed()**"
      ],
      "metadata": {
        "id": "TrD0mvgnLMw3"
      },
      "id": "TrD0mvgnLMw3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5432faa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63aa097-d1ef-442b-e14a-fd528ccdff2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+----------+------+\n",
            "|New Name|age|Experience|Salary|\n",
            "+--------+---+----------+------+\n",
            "| Sourabh| 31|        10| 30000|\n",
            "|   Disha| 30|         8| 25000|\n",
            "|   Sunny| 29|         4| 20000|\n",
            "|    Paul| 24|         3| 20000|\n",
            "|  Harsha| 21|         1| 15000|\n",
            "| Shubham| 23|         2| 18000|\n",
            "+--------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.withColumnRenamed('Name','New Name').show()\n"
      ],
      "id": "5432faa1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with DataFrames: Reading CSV, Inspecting Schema, and Dropping a Column"
      ],
      "metadata": {
        "id": "BEcJu3rH8qS5"
      },
      "id": "BEcJu3rH8qS5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "805e7382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bad9603-8c7c-45c5-c3a9-3124cea5cb50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "+-------+----+----------+------+\n",
            "|   Name| age|Experience|Salary|\n",
            "+-------+----+----------+------+\n",
            "|Sourabh|  31|        10| 30000|\n",
            "|  Disha|  30|         8| 25000|\n",
            "|  Sunny|  29|         4| 20000|\n",
            "|   Paul|  24|         3| 20000|\n",
            "| Harsha|  21|         1| 15000|\n",
            "|Shubham|  23|         2| 18000|\n",
            "| Mahesh|NULL|      NULL| 40000|\n",
            "|   NULL|  34|        10| 38000|\n",
            "|   NULL|  36|      NULL|  NULL|\n",
            "+-------+----+----------+------+\n",
            "\n",
            "+----+----------+------+\n",
            "| age|Experience|Salary|\n",
            "+----+----------+------+\n",
            "|  31|        10| 30000|\n",
            "|  30|         8| 25000|\n",
            "|  29|         4| 20000|\n",
            "|  24|         3| 20000|\n",
            "|  21|         1| 15000|\n",
            "|  23|         2| 18000|\n",
            "|NULL|      NULL| 40000|\n",
            "|  34|        10| 38000|\n",
            "|  36|      NULL|  NULL|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Practice').getOrCreate()\n",
        "\n",
        "df_pyspark=spark.read.csv('test2.csv', header=True, inferSchema=True )\n",
        "df_pyspark.printSchema()\n",
        "df_pyspark.show()\n",
        "\n",
        "df_pyspark.drop('Name').show()"
      ],
      "id": "805e7382"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the DataFrame"
      ],
      "metadata": {
        "id": "17efd1R-DNsm"
      },
      "id": "17efd1R-DNsm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c041e07"
      },
      "outputs": [],
      "source": [
        "df_pyspark.show()"
      ],
      "id": "9c041e07"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **df_pyspark.na.drop()** - Dropping Rows with All Null Values"
      ],
      "metadata": {
        "id": "lIy_PE0tNUss"
      },
      "id": "lIy_PE0tNUss"
    },
    {
      "cell_type": "code",
      "source": [
        "### how == all\n",
        "df_pyspark.na.drop(how=\"all\").show() #drop the rows  if all entries are null in that row."
      ],
      "metadata": {
        "id": "PK6Q-IyV-QNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a819f29-f1c4-4128-c672-25159b3cab2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+------+\n",
            "|   Name| age|Experience|Salary|\n",
            "+-------+----+----------+------+\n",
            "|Sourabh|  31|        10| 30000|\n",
            "|  Disha|  30|         8| 25000|\n",
            "|  Sunny|  29|         4| 20000|\n",
            "|   Paul|  24|         3| 20000|\n",
            "| Harsha|  21|         1| 15000|\n",
            "|Shubham|  23|         2| 18000|\n",
            "| Mahesh|NULL|      NULL| 40000|\n",
            "|   NULL|  34|        10| 38000|\n",
            "|   NULL|  36|      NULL|  NULL|\n",
            "+-------+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "id": "PK6Q-IyV-QNK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping Rows with Any Null Value"
      ],
      "metadata": {
        "id": "YnkECExEDcXf"
      },
      "id": "YnkECExEDcXf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "156e41cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8941fb-c62c-4bf7-fa32-94a42642d22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### how ==any\n",
        "df_pyspark.na.drop(how=\"any\").show() #drop the rows even if there is a single null value."
      ],
      "id": "156e41cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Missing Values Using PySpark's Imputer\n"
      ],
      "metadata": {
        "id": "Ol5ZS_R8Dmy3"
      },
      "id": "Ol5ZS_R8Dmy3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e31190f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b019365f-1222-4105-fc91-56d56819ded4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+------+-----------+------------------+--------------+\n",
            "|   Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
            "+-------+----+----------+------+-----------+------------------+--------------+\n",
            "|Sourabh|  31|        10| 30000|         31|                10|         30000|\n",
            "|  Disha|  30|         8| 25000|         30|                 8|         25000|\n",
            "|  Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
            "|   Paul|  24|         3| 20000|         24|                 3|         20000|\n",
            "| Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
            "|Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
            "| Mahesh|NULL|      NULL| 40000|         29|                 4|         40000|\n",
            "|   NULL|  34|        10| 38000|         34|                10|         38000|\n",
            "|   NULL|  36|      NULL|  NULL|         36|                 4|         20000|\n",
            "+-------+----+----------+------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=['age', 'Experience', 'Salary'],\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
        "    ).setStrategy(\"median\")\n",
        "\n",
        "# Add imputation cols to df\n",
        "imputer.fit(df_pyspark).transform(df_pyspark).show()"
      ],
      "id": "e31190f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d6364f6"
      },
      "source": [
        "#  Pyspark Dataframes"
      ],
      "id": "5d6364f6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "inferSchema=True,  -- automatically examine the data and guess the correct data types for each column."
      ],
      "metadata": {
        "id": "1tvLxh1ExIsr"
      },
      "id": "1tvLxh1ExIsr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading and Displaying Data from a CSV File"
      ],
      "metadata": {
        "id": "H2Td7SKsEvhW"
      },
      "id": "H2Td7SKsEvhW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8d843e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e6459d-0e7e-4fe8-a545-38d10d3f478e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('dataframe').getOrCreate()\n",
        "\n",
        "df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)\n",
        "df_pyspark.show()\n"
      ],
      "id": "d8d843e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0fdbb15"
      },
      "source": [
        "### Filtering Data Based on a Condition"
      ],
      "id": "e0fdbb15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c21edffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314b6ac0-c2da-48f7-fba5-c199519f48bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.filter(\"Salary<=20000\").show()\n"
      ],
      "id": "c21edffc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering and Selecting Specific Columns"
      ],
      "metadata": {
        "id": "3m23gRvUE9J0"
      },
      "id": "3m23gRvUE9J0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5a5f3af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65530ab4-11de-4dff-9caa-913b8819e1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|age|\n",
            "+-------+---+\n",
            "|  Sunny| 29|\n",
            "|   Paul| 24|\n",
            "| Harsha| 21|\n",
            "|Shubham| 23|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.filter(\"Salary<=20000\").select(['Name','age']).show()"
      ],
      "id": "d5a5f3af"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Data with Multiple Conditions"
      ],
      "metadata": {
        "id": "uEZMSF2aFByW"
      },
      "id": "uEZMSF2aFByW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26f76ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ec6104-84e1-44e7-c839-928a57246520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.filter((df_pyspark['Salary']<=20000) |\n",
        "                  (df_pyspark['Salary']>=15000)).show()"
      ],
      "id": "26f76ee1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Data with Multiple Conditions Using AND"
      ],
      "metadata": {
        "id": "w8p-GsU7FHsN"
      },
      "id": "w8p-GsU7FHsN"
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.filter((df_pyspark['Salary']<=20000) &\n",
        "                  (df_pyspark['Salary']>=15000)).show()"
      ],
      "metadata": {
        "id": "Sh8zHaBuLV5T"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Sh8zHaBuLV5T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2efa0fce"
      },
      "source": [
        "### Creating a SparkSession for Aggregations"
      ],
      "id": "2efa0fce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f336300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "9f950ba0-cc8c-40a3-fbf7-515a09771dc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79f6d12be450>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c3d98e63fb21:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Practise</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Agg').getOrCreate()\n",
        "spark\n"
      ],
      "id": "8f336300"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Data from a CSV File"
      ],
      "metadata": {
        "id": "MhdPIYhdFQgV"
      },
      "id": "MhdPIYhdFQgV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ed791ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686004fd-f8a2-4c1e-bd8b-7f3052b9acbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+------+\n",
            "|     Name| Departments|salary|\n",
            "+---------+------------+------+\n",
            "|  Sourabh|Data Science| 10000|\n",
            "|  Sourabh|         IOT|  5000|\n",
            "|   Mahesh|    Big Data|  4000|\n",
            "|  Sourabh|    Big Data|  4000|\n",
            "|   Mahesh|Data Science|  3000|\n",
            "|Sudhanshu|Data Science| 20000|\n",
            "|Sudhanshu|         IOT| 10000|\n",
            "|Sudhanshu|    Big Data|  5000|\n",
            "|    Sunny|Data Science| 10000|\n",
            "|    Sunny|    Big Data|  2000|\n",
            "+---------+------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)\n",
        "df_pyspark.show()"
      ],
      "id": "7ed791ed"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing the Schema of a DataFrame"
      ],
      "metadata": {
        "id": "AygJnO5QFs9E"
      },
      "id": "AygJnO5QFs9E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d57d24ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13340084-49a1-4975-a456-a5fea1939254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Departments: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.printSchema()"
      ],
      "id": "d57d24ca"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping Data by a Column"
      ],
      "metadata": {
        "id": "F6hXcuqFFxOT"
      },
      "id": "F6hXcuqFFxOT"
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.groupBy('Name')"
      ],
      "metadata": {
        "id": "3E-jdphhOnSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a37d1f-bb08-4f7c-b7dd-51813646625b"
      },
      "id": "3E-jdphhOnSO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GroupedData[grouping expressions: [Name], value: [Name: string, Departments: string ... 1 more field], type: GroupBy]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping Data and Summing Values"
      ],
      "metadata": {
        "id": "L8_X2S2nGqWM"
      },
      "id": "L8_X2S2nGqWM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f15f8197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b770d82a-7fa8-48aa-974d-0a6c38cf519e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     Name|sum(salary)|\n",
            "+---------+-----------+\n",
            "|Sudhanshu|      35000|\n",
            "|  Sourabh|      19000|\n",
            "|    Sunny|      12000|\n",
            "|   Mahesh|       7000|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Groupby\n",
        "df_pyspark.groupBy('Name').sum().show()"
      ],
      "id": "f15f8197"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping Data and Calculating Average"
      ],
      "metadata": {
        "id": "ZuMut2KuGu26"
      },
      "id": "ZuMut2KuGu26"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc122ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84f7a12-58a0-4282-e6f7-c61089412da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+\n",
            "|     Name|       avg(salary)|\n",
            "+---------+------------------+\n",
            "|Sudhanshu|11666.666666666666|\n",
            "|  Sourabh| 6333.333333333333|\n",
            "|    Sunny|            6000.0|\n",
            "|   Mahesh|            3500.0|\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.groupBy('Name').avg().show()"
      ],
      "id": "fc122ace"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping Data and Summing Values by Department"
      ],
      "metadata": {
        "id": "TkeR5IxSGzLx"
      },
      "id": "TkeR5IxSGzLx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "151d2264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbf44a4-a0c5-4020-e271-5679f9b4ce21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "| Departments|sum(salary)|\n",
            "+------------+-----------+\n",
            "|         IOT|      15000|\n",
            "|    Big Data|      15000|\n",
            "|Data Science|      43000|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df_pyspark.groupBy('Departments').sum().show()"
      ],
      "id": "151d2264"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16da6c54"
      },
      "source": [
        "# Examples Of Pyspark ML"
      ],
      "id": "16da6c54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://spark.apache.org/examples.html"
      ],
      "metadata": {
        "id": "ONZ4gb7fMaA1"
      },
      "id": "ONZ4gb7fMaA1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a SparkSession for Handling Missing Data"
      ],
      "metadata": {
        "id": "zTFgZRJZG-4l"
      },
      "id": "zTFgZRJZG-4l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b9da3ad"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Missing').getOrCreate()\n"
      ],
      "id": "0b9da3ad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the Dataset and Inspecting It"
      ],
      "metadata": {
        "id": "H7FyoccoHDrT"
      },
      "id": "H7FyoccoHDrT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "735525da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e984d5c7-28a3-4a61-895a-6e3fbecc27a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|Sourabh| 31|        10| 30000|\n",
            "|  Disha| 30|         8| 25000|\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Read The dataset\n",
        "training = spark.read.csv('test1.csv',header=True,inferSchema=True)\n",
        "training.show()\n",
        "training.printSchema()\n"
      ],
      "id": "735525da"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Column Names of a DataFrame"
      ],
      "metadata": {
        "id": "qSNFk-8hHITp"
      },
      "id": "qSNFk-8hHITp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d3227e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cf3986-4b62-490c-c171-0eb07b34f7c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Name', 'age', 'Experience', 'Salary']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "training.columns"
      ],
      "id": "5d3227e6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assembling Features into a Vector"
      ],
      "metadata": {
        "id": "lXK_Gjo8IBnT"
      },
      "id": "lXK_Gjo8IBnT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6273555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0456a3-6690-49ef-cac6-8c7ba3596dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+--------------------+\n",
            "|   Name|age|Experience|Salary|Independent Features|\n",
            "+-------+---+----------+------+--------------------+\n",
            "|Sourabh| 31|        10| 30000|         [31.0,10.0]|\n",
            "|  Disha| 30|         8| 25000|          [30.0,8.0]|\n",
            "|  Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
            "|   Paul| 24|         3| 20000|          [24.0,3.0]|\n",
            "| Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
            "|Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
            "+-------+---+----------+------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "featureassembler = VectorAssembler(\n",
        "                        inputCols= [\"age\",\"Experience\"],\n",
        "                        outputCol= \"Independent Features\")\n",
        "output = featureassembler.transform(training)\n",
        "output.show()"
      ],
      "id": "e6273555"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c27434a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2c29e1-0066-4857-a281-a4f220822f6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Name', 'age', 'Experience', 'Salary', 'Independent Features']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "output.columns"
      ],
      "id": "2c27434a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting Specific Columns from a DataFrame\n"
      ],
      "metadata": {
        "id": "H64rriykILrC"
      },
      "id": "H64rriykILrC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54a0ccab"
      },
      "outputs": [],
      "source": [
        "finalized_data = output.select(\"Independent Features\",\"Salary\")"
      ],
      "id": "54a0ccab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7a73845",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5949afa8-f03c-4237-96bf-d1036af98da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|Independent Features|Salary|\n",
            "+--------------------+------+\n",
            "|         [31.0,10.0]| 30000|\n",
            "|          [30.0,8.0]| 25000|\n",
            "|          [29.0,4.0]| 20000|\n",
            "|          [24.0,3.0]| 20000|\n",
            "|          [21.0,1.0]| 15000|\n",
            "|          [23.0,2.0]| 18000|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "finalized_data.show()"
      ],
      "id": "f7a73845"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Data and Fitting a Linear Regression Model\n"
      ],
      "metadata": {
        "id": "b8oKMWI_IpDZ"
      },
      "id": "b8oKMWI_IpDZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b11192b"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "train_data,test_data = finalized_data.randomSplit([0.75,0.25])\n",
        "\n",
        "regressor=LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
        "regressor=regressor.fit(train_data)\n"
      ],
      "id": "0b11192b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing the Coefficients of a Linear Regression Model\n"
      ],
      "metadata": {
        "id": "p7qUoF_FItfJ"
      },
      "id": "p7qUoF_FItfJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa4ec997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbec4a46-d608-40a0-f2d7-7db369cebbaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([-64.8464, 1584.7554])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "regressor.coefficients"
      ],
      "id": "fa4ec997"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing the Intercept of a Linear Regression Model\n"
      ],
      "metadata": {
        "id": "RAtkuQzhIxv-"
      },
      "id": "RAtkuQzhIxv-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eba911b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bc64fc-2efa-4c89-d4f8-1cb0f6c20120"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15414.10693970355"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "regressor.intercept"
      ],
      "id": "eba911b6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Linear Regression Model on Test Data\n"
      ],
      "metadata": {
        "id": "jUTLQsTeI2ob"
      },
      "id": "jUTLQsTeI2ob"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ba2bc70"
      },
      "outputs": [],
      "source": [
        "pred_results=regressor.evaluate(test_data)"
      ],
      "id": "2ba2bc70"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Predictions from the Model\n"
      ],
      "metadata": {
        "id": "qcdhrTp0I7az"
      },
      "id": "qcdhrTp0I7az"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "489d6392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022e2e12-6802-431c-c8c7-d7dee9b9058b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+------------------+\n",
            "|Independent Features|Salary|        prediction|\n",
            "+--------------------+------+------------------+\n",
            "|          [24.0,3.0]| 20000|18612.059158134216|\n",
            "+--------------------+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred_results.predictions.show()"
      ],
      "id": "489d6392"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Spark SQL\n",
        "- - -\n",
        "\n",
        "- Spark SQL **provides a DataFrame API** that can perform 'relational operations on both external data sources and Spark's built-in distributed collections' —at scale!\n",
        "\n",
        "- It support a wide variety of  data sources and algorithms in Big Data, which makes it easy to\n",
        "  - **add data sources**,\n",
        "  - **optimization rules**, and\n",
        "  - **data types** for advanced analytics such as machine learning.\n",
        "\n",
        "- Spark SQL provides\n",
        "  - **state-of-the-art SQL performance** and\n",
        "  - **maintains compatibility with all existing structures and components**"
      ],
      "metadata": {
        "id": "DC3HW4pNMWML"
      },
      "id": "DC3HW4pNMWML"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUMgbl49LxaD"
      },
      "source": [
        "### Useful references for this Notebook\n",
        "* [PySpark in Jupyter Notebook — Working with Dataframe & JDBC Data Sources](https://medium.com/@thucnc/pyspark-in-jupyter-notebook-working-with-dataframe-jdbc-data-sources-6f3d39300bf6)\n",
        "* [PySpark - Working with JDBC Sqlite database](http://mitzen.blogspot.com/2017/06/pyspark-working-with-jdbc-sqlite.html)"
      ],
      "id": "pUMgbl49LxaD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg7N1fDbLxaE"
      },
      "source": [
        "### Create a SparkSession and read the a stock price dataset CSV"
      ],
      "id": "yg7N1fDbLxaE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing PySpark\n"
      ],
      "metadata": {
        "id": "8hKoLz8HJIhf"
      },
      "id": "8hKoLz8HJIhf"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igHcmQEuP51F",
        "outputId": "49f41526-8fec-4fdc-bd75-0936abb8a894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "id": "igHcmQEuP51F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing PySpark SQL Functions\n"
      ],
      "metadata": {
        "id": "U_U4ACjmJT8x"
      },
      "id": "U_U4ACjmJT8x"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max,min,first,last,kurtosis,mean,skewness"
      ],
      "metadata": {
        "id": "d1wo0gVTIJSw"
      },
      "id": "d1wo0gVTIJSw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Additional PySpark SQL Functions\n"
      ],
      "metadata": {
        "id": "J_jOPVGqJZeX"
      },
      "id": "J_jOPVGqJZeX"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import stddev,stddev_samp,stddev_pop,sum,sumDistinct,variance,var_samp,var_pop,sum"
      ],
      "metadata": {
        "id": "w7CHLdcyIQqQ"
      },
      "id": "w7CHLdcyIQqQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a SparkSession for SQL Operations\n"
      ],
      "metadata": {
        "id": "MyzFcfJ3Jj_q"
      },
      "id": "MyzFcfJ3Jj_q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2zfMwvBLxaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "9001c42c-21b4-4b50-8eb2-4c0794eec906"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7954cc5ec350>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c3d98e63fb21:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>HappyLearning</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "from pyspark import SparkContext as sc\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark1 = SparkSession.builder.appName('SQL').getOrCreate()\n",
        "spark1"
      ],
      "id": "G2zfMwvBLxaE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Data from a CSV File into a DataFrame\n"
      ],
      "metadata": {
        "id": "lsRnRf2uJqE-"
      },
      "id": "lsRnRf2uJqE-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUUfeTmRLxaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8f1b63-72b3-4140-9c48-eced9645bf8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "df = spark1.read.csv('appl_stock.csv', inferSchema=True, header=True )\n",
        "df"
      ],
      "id": "RUUfeTmRLxaI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing the Schema of a DataFrame\n"
      ],
      "metadata": {
        "id": "Q8T3L4U2JvfR"
      },
      "id": "Q8T3L4U2JvfR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjVakenRLxaJ",
        "outputId": "a3f38150-3498-45c2-9867-d9cfcf23968a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ],
      "id": "sjVakenRLxaJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIDof9WDLxaK"
      },
      "source": [
        "### Creating or Replacing a Temporary View\n"
      ],
      "id": "JIDof9WDLxaK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQdrnnZVLxaK"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView('stock')"
      ],
      "id": "gQdrnnZVLxaK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbXRdaK_LxaL"
      },
      "source": [
        "Now run a simple SQL query directly on this view. It returns a DataFrame.\n",
        "\n",
        "LIMIT 5 → Restrict the result to just 5 rows ("
      ],
      "id": "SbXRdaK_LxaL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running a SQL Query on the Temporary View\n"
      ],
      "metadata": {
        "id": "MdcN1WPqKOdG"
      },
      "id": "MdcN1WPqKOdG"
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL\n",
        "- SELECT * FROM stock"
      ],
      "metadata": {
        "id": "lxffDxtXpBx1"
      },
      "id": "lxffDxtXpBx1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running an SQL Query with LIMIT on the Temporary View\n"
      ],
      "metadata": {
        "id": "5yGLiud4KTpy"
      },
      "id": "5yGLiud4KTpy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chZqlZpnLxaL",
        "outputId": "fbd70e49-6006-4a8f-c3e9-f12a579abe60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "result = spark1.sql(\"SELECT * FROM stock LIMIT 5\")\n",
        "result"
      ],
      "id": "chZqlZpnLxaL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the Column Names of the Resulting DataFrame\n"
      ],
      "metadata": {
        "id": "7tSEMGmnKYkt"
      },
      "id": "7tSEMGmnKYkt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ3lLx6ALxaM",
        "outputId": "c9c54bd3-08ee-4be2-c0c8-edfe17ff26d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "result.columns"
      ],
      "id": "fQ3lLx6ALxaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2TsqAL5LxaM",
        "outputId": "f36f6096-6740-41b2-b893-cecbf74c514a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
            "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
            "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
            "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
            "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
            "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
            "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.show()"
      ],
      "id": "m2TsqAL5LxaM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxUG_IbuLxaN"
      },
      "source": [
        "### Run slightly more complex queries\n",
        "How many entries in the `Close` field are higher than 500?"
      ],
      "id": "PxUG_IbuLxaN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting Rows Based on a Condition in SQL\n"
      ],
      "metadata": {
        "id": "oPhdNFTyKfz3"
      },
      "id": "oPhdNFTyKfz3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-CVCSwaLxaN",
        "outputId": "63654596-7b74-4840-e1ed-b77b94dcb3f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|count(Close)|\n",
            "+------------+\n",
            "|         403|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "count_greater_500 = spark1.sql(\"SELECT COUNT(Close) FROM stock WHERE Close > 500\").show()"
      ],
      "id": "f-CVCSwaLxaN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9nTyzSiLxaR"
      },
      "source": [
        "What is the average `Open` values of all the entries where `Volume` is either greater than 120 million or less than 110 million?"
      ],
      "id": "S9nTyzSiLxaR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the Average with a Conditional Filter\n"
      ],
      "metadata": {
        "id": "bzfn1caJKlml"
      },
      "id": "bzfn1caJKlml"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY3bAWwoLxaR",
        "outputId": "431c2ccf-f49a-4967-d027-33c6bd019b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|         avg(Open)|\n",
            "+------------------+\n",
            "|309.12406365290224|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_1 = spark1.sql(\"SELECT AVG(Open) FROM stock WHERE Volume > 120000000 OR Volume < 110000000\").show()"
      ],
      "id": "AY3bAWwoLxaR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing SparkSession\n"
      ],
      "metadata": {
        "id": "z-9c7bC3KsR9"
      },
      "id": "z-9c7bC3KsR9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName('app').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "J7w-PJZ3GnfB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "9e50e229-de7d-4024-e95f-51ebcc069162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bbe1af70970>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d6d2945b49f2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SQL</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "id": "J7w-PJZ3GnfB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Data and Creating Table View"
      ],
      "metadata": {
        "id": "6mWLmVSktAZn"
      },
      "id": "6mWLmVSktAZn"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "mtcars = pd.read_csv('mtcars.csv')\n",
        "mtcars.head()\n"
      ],
      "metadata": {
        "id": "FpJhLhIxGl6Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e40b7148-8a98-4ee4-d7fb-814a1b0ea470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
              "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
              "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
              "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
              "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
              "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
              "\n",
              "   carb  \n",
              "0     4  \n",
              "1     4  \n",
              "2     1  \n",
              "3     1  \n",
              "4     2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cacc17b3-4db2-45da-95b2-5502aee4cc8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>mpg</th>\n",
              "      <th>cyl</th>\n",
              "      <th>disp</th>\n",
              "      <th>hp</th>\n",
              "      <th>drat</th>\n",
              "      <th>wt</th>\n",
              "      <th>qsec</th>\n",
              "      <th>vs</th>\n",
              "      <th>am</th>\n",
              "      <th>gear</th>\n",
              "      <th>carb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mazda RX4</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.90</td>\n",
              "      <td>2.620</td>\n",
              "      <td>16.46</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mazda RX4 Wag</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.90</td>\n",
              "      <td>2.875</td>\n",
              "      <td>17.02</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Datsun 710</td>\n",
              "      <td>22.8</td>\n",
              "      <td>4</td>\n",
              "      <td>108.0</td>\n",
              "      <td>93</td>\n",
              "      <td>3.85</td>\n",
              "      <td>2.320</td>\n",
              "      <td>18.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hornet 4 Drive</td>\n",
              "      <td>21.4</td>\n",
              "      <td>6</td>\n",
              "      <td>258.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.215</td>\n",
              "      <td>19.44</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hornet Sportabout</td>\n",
              "      <td>18.7</td>\n",
              "      <td>8</td>\n",
              "      <td>360.0</td>\n",
              "      <td>175</td>\n",
              "      <td>3.15</td>\n",
              "      <td>3.440</td>\n",
              "      <td>17.02</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cacc17b3-4db2-45da-95b2-5502aee4cc8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cacc17b3-4db2-45da-95b2-5502aee4cc8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cacc17b3-4db2-45da-95b2-5502aee4cc8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a797e378-9e55-4f42-8a3e-1733cd2428bc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a797e378-9e55-4f42-8a3e-1733cd2428bc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a797e378-9e55-4f42-8a3e-1733cd2428bc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "id": "FpJhLhIxGl6Z"
    },
    {
      "cell_type": "code",
      "source": [
        "mtcars.rename( columns={'Unnamed: 0':'name'}, inplace=True )"
      ],
      "metadata": {
        "id": "L5Ss58B9tIum"
      },
      "execution_count": null,
      "outputs": [],
      "id": "L5Ss58B9tIum"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataframe into Spark\n"
      ],
      "metadata": {
        "id": "pykTjsN-tNv5"
      },
      "id": "pykTjsN-tNv5"
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = spark.createDataFrame(mtcars)"
      ],
      "metadata": {
        "id": "9G3hwslgtK8r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9G3hwslgtK8r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Table View"
      ],
      "metadata": {
        "id": "u0eCK9LstTpz"
      },
      "id": "u0eCK9LstTpz"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM cars\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3_eydS4tZp7",
        "outputId": "53fc2292-147a-4401-9fa7-6bd35145fd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|         Unnamed: 0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "id": "r3_eydS4tZp7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Selecting a Specific Table"
      ],
      "metadata": {
        "id": "F0rwAOC1thyI"
      },
      "id": "F0rwAOC1thyI"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT mpg FROM cars\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3xmXluBtfNf",
        "outputId": "b0bd6f1c-d03e-4ee2-ed0c-f988047f9eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "| mpg|\n",
            "+----+\n",
            "|21.0|\n",
            "|21.0|\n",
            "|22.8|\n",
            "|21.4|\n",
            "|18.7|\n",
            "+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "id": "t3xmXluBtfNf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic SQL filtering"
      ],
      "metadata": {
        "id": "04pPoGJJtm_a"
      },
      "id": "04pPoGJJtm_a"
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic filtering query to determine cards that have a high mileage and low cylinder count\n",
        "\n",
        "spark.sql (\"SELECT* FROM cars where mpg>20 AND cyl<6\"). show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvRjBnD6tkeQ",
        "outputId": "df33bc02-5c7c-4122-f475-bc1551c8b4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "| Unnamed: 0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "| Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
            "|  Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
            "|   Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
            "|   Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
            "|Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
            "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "id": "yvRjBnD6tkeQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregation"
      ],
      "metadata": {
        "id": "Utyrc6uOt6zo"
      },
      "id": "Utyrc6uOt6zo"
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregating data and grouping by cylinders\n",
        "spark.sql (\"SELECT count (*), cyl from cars GROUP BY cyl\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaxbmjjYtpsz",
        "outputId": "c1021514-e2ff-47ec-835d-3e66692f0d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+\n",
            "|count(1)|cyl|\n",
            "+--------+---+\n",
            "|       7|  6|\n",
            "|      14|  8|\n",
            "|      11|  4|\n",
            "+--------+---+\n",
            "\n"
          ]
        }
      ],
      "id": "VaxbmjjYtpsz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grouping function"
      ],
      "metadata": {
        "id": "pZAdqfPrpOxY"
      },
      "id": "pZAdqfPrpOxY"
    },
    {
      "cell_type": "code",
      "source": [
        "df.cube(\"Department\", \"employee_name\").agg(grouping(\"Department\").alias(\"is_dept_grouped\")).show()"
      ],
      "metadata": {
        "id": "1is9ooPspMfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56456809-979e-4cd5-b2d9-32328e23892e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------------+\n",
            "|Department|employee_name|is_dept_grouped|\n",
            "+----------+-------------+---------------+\n",
            "|     Sales|        Jonny|              0|\n",
            "|     Sales|       Rajesh|              0|\n",
            "|     Sales|        Sahil|              0|\n",
            "|   Finance|        Sania|              0|\n",
            "|     Sales|       Aakash|              0|\n",
            "|     Sales|         Saif|              0|\n",
            "| Marketing|        Ankit|              0|\n",
            "| Marketing|        Kumar|              0|\n",
            "|   Finance|        Tarun|              0|\n",
            "|   Finance|         Ajay|              0|\n",
            "|   Finance|         NULL|              0|\n",
            "|      NULL|       Aakash|              1|\n",
            "|      NULL|         NULL|              1|\n",
            "|      NULL|        Jonny|              1|\n",
            "|      NULL|        Sania|              1|\n",
            "|      NULL|       Rajesh|              1|\n",
            "|      NULL|        Sahil|              1|\n",
            "|     Sales|         NULL|              0|\n",
            "| Marketing|         NULL|              0|\n",
            "|      NULL|         Saif|              1|\n",
            "+----------+-------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "id": "1is9ooPspMfS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###first function\n",
        "returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element."
      ],
      "metadata": {
        "id": "hpB0nRM-qZps"
      },
      "id": "hpB0nRM-qZps"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(first(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xql9Ht9pjVi",
        "outputId": "f345d43b-63aa-4ec5-e14a-009ff671dd1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|first(salary)|\n",
            "+-------------+\n",
            "|3000         |\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "id": "_Xql9Ht9pjVi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###last function\n",
        "last() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element."
      ],
      "metadata": {
        "id": "0GW6tLBuqds0"
      },
      "id": "0GW6tLBuqds0"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(last(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhQ1IYvIqbuN",
        "outputId": "ced41741-84a0-4e77-adfb-c703fe170bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|last(salary)|\n",
            "+------------+\n",
            "|4100        |\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "id": "XhQ1IYvIqbuN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###kurtosis function:\n",
        "\n",
        "- Kurtosis is a statistical measure that describes the shape, or \"tailedness,\" of a distribution.\n",
        "- kurtosis() function returns the kurtosis of the values in a group.\n"
      ],
      "metadata": {
        "id": "9UQY7vV0qhtn"
      },
      "id": "9UQY7vV0qhtn"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(kurtosis(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cIWgGBBqfoX",
        "outputId": "30a28415-5ccc-4c55-aa98-69d287e8e089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|kurtosis(salary)   |\n",
            "+-------------------+\n",
            "|-0.6467803030303032|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "id": "2cIWgGBBqfoX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###max function\n",
        "- returns the maximum value in a column."
      ],
      "metadata": {
        "id": "3WCmQ9q-qnHR"
      },
      "id": "3WCmQ9q-qnHR"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(max(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXx06MfoqlTu",
        "outputId": "0e66af20-6121-4cbd-c95f-ecd17abf95f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|max(salary)|\n",
            "+-----------+\n",
            "|4600       |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "id": "BXx06MfoqlTu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###min function"
      ],
      "metadata": {
        "id": "pppvuVzCqq6g"
      },
      "id": "pppvuVzCqq6g"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(min(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0xiqnvGqpTP",
        "outputId": "143c24e6-6efe-4b48-8a64-3e53cd37b37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|min(salary)|\n",
            "+-----------+\n",
            "|2000       |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "id": "i0xiqnvGqpTP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###mean function"
      ],
      "metadata": {
        "id": "VNaOaCCcquzh"
      },
      "id": "VNaOaCCcquzh"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(mean(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYYIhLGfqs1V",
        "outputId": "e884e659-2b1d-4d52-88f5-8e5a583cddd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|avg(salary)|\n",
            "+-----------+\n",
            "|3400.0     |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "id": "DYYIhLGfqs1V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###skewness function:\n",
        "- It provides **information about the lack of symmetry** in the distribution.\n",
        "- skewness() function **returns the skewness of the values** in a group."
      ],
      "metadata": {
        "id": "5SD_T1iHqy7N"
      },
      "id": "5SD_T1iHqy7N"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(skewness(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4XN2SmMqxYd",
        "outputId": "fa99cc37-d290-4943-df9e-1d64f114d377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|skewness(salary)    |\n",
            "+--------------------+\n",
            "|-0.12041791181069571|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "id": "-4XN2SmMqxYd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###stddev(), stddev_samp() and stddev_pop()\n",
        "- stddev() alias for stddev_samp.\n",
        "\n",
        "- stddev_samp() function returns the sample standard deviation of values in a column.\n",
        "\n",
        "- stddev_pop() function returns the population standard deviation of the values in a column."
      ],
      "metadata": {
        "id": "n6cny4l5q3IA"
      },
      "id": "n6cny4l5q3IA"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), stddev_pop(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGKWfzfgq1K9",
        "outputId": "b69befa7-2a67-445c-9875-2251200125c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+------------------+\n",
            "|stddev(salary)   |stddev_samp(salary)|stddev_pop(salary)|\n",
            "+-----------------+-------------------+------------------+\n",
            "|765.9416862050705|765.9416862050705  |726.636084983398  |\n",
            "+-----------------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "id": "TGKWfzfgq1K9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum function\n",
        "Returns the sum of all values in a column."
      ],
      "metadata": {
        "id": "gjhGipWcq-tr"
      },
      "id": "gjhGipWcq-tr"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(sum(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmK9Hhs9q9BR",
        "outputId": "1b0626c4-393c-42fb-b2db-332e738fc7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|sum(salary)|\n",
            "+-----------+\n",
            "|34000      |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "id": "YmK9Hhs9q9BR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sumDistinct function\n",
        "returns the **sum of all distinct values** in a column."
      ],
      "metadata": {
        "id": "MO3Ro2N5rCd1"
      },
      "id": "MO3Ro2N5rCd1"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select( sumDistinct(\"salary\") ).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ5pe_murA2U",
        "outputId": "2ff8439e-9939-4459-b353-f967815be022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
            "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|sum(DISTINCT salary)|\n",
            "+--------------------+\n",
            "|20900               |\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "id": "WQ5pe_murA2U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###variance(), var_samp(), var_pop()\n",
        "- variance() alias for var_samp\n",
        "- var_samp() function -  returns the unbiased variance of the values in a column.\n",
        "- var_pop() function - returns the population variance of the values in a column."
      ],
      "metadata": {
        "id": "YdtecR0drFy-"
      },
      "id": "YdtecR0drFy-"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "w9DQKCUwrES4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8938e597-3c50-411e-f9c6-ea6aab07c290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+---------------+\n",
            "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
            "+-----------------+-----------------+---------------+\n",
            "|586666.6666666666|586666.6666666666|528000.0       |\n",
            "+-----------------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "id": "w9DQKCUwrES4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ctxuKVgtJ-p",
        "outputId": "be45fed1-e7c0-481d-eee1-57dae0f1f5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ],
      "id": "7ctxuKVgtJ-p"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "2FA5jwpMtVmV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2FA5jwpMtVmV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PySpark SQL Date Functions"
      ],
      "metadata": {
        "id": "A-NrEvyTtZ6T"
      },
      "id": "A-NrEvyTtZ6T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| PYSPARK DATE FUNCTION   | DATE FUNCTION DESCRIPTION                                                                                   |\n",
        "|-------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| current_date()          | Returns the current date as a date column.                                                                  |\n",
        "| date_format(dateExpr, format) | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. |\n",
        "| to_date()               | Converts the column into `DateType` by casting rules to `DateType`.                                         |\n",
        "| to_date(column, fmt)    | Converts the column into a `DateType` with a specified format.                                               |\n",
        "| add_months(Column, numMonths) | Returns the date that is `numMonths` after `startDate`.                                                      |\n",
        "| date_add(column, days)  | Returns the date that is `days` days after `start`.                                                          |\n",
        "| date_sub(column, days)  | Returns the date that is `days` days before `start`.                                                         |\n",
        "| datediff(end, start)    | Returns the number of days from `start` to `end`.                                                            |\n",
        "| months_between(end, start) | Returns the number of months between dates `start` and `end`.                                                |\n",
        "| months_between(end, start, roundOff) | Returns the number of months between dates `end` and `start`. If `roundOff` is set to true, the result is rounded off to 8 digits; otherwise, it is not rounded. |\n",
        "| next_day(column, dayOfWeek) | Returns the first date which is later than the value of the `date` column that is on the specified day of the week. |\n",
        "| trunc(column, format)   | Returns date truncated to the unit specified by the format.                                                  |\n",
        "| date_trunc(format, timestamp) | Returns timestamp truncated to the unit specified by the format.                                             |\n",
        "| year(column)            | Extracts the year as an integer from a given date/timestamp/string.                                          |\n",
        "| quarter(column)         | Extracts the quarter as an integer from a given date/timestamp/string.                                       |\n",
        "| month(column)           | Extracts the month as an integer from a given date/timestamp/string.                                         |\n",
        "| dayofweek(column)       | Extracts the day of the week as an integer from a given date/timestamp/string. Ranges from 1 for Sunday through 7 for Saturday. |\n",
        "| dayofmonth(column)      | Extracts the day of the month as an integer from a given date/timestamp/string.                              |\n",
        "| dayofyear(column)       | Extracts the day of the year as an integer from a given date/timestamp/string.                               |\n",
        "| weekofyear(column)      | Extracts the week number as an integer from a given date/timestamp/string. A week is considered to start on a Monday, and week 1 is the first week with more than 3 days, as defined by ISO 8601. |\n",
        "| last_day(column)        | Returns the last day of the month which the given date belongs to. For example, input \"2015-07-27\" returns \"2015-07-31\" since July 31 is the last day of the month in July 2015. |\n",
        "| from_unixtime(column)   | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format. |\n",
        "| from_unixtime(column, f) | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. |\n",
        "| unix_timestamp()        | Returns the current Unix timestamp (in seconds) as a long.                                                  |\n",
        "| unix_timestamp(column)  | Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale. |\n",
        "| unix_timestamp(column, p) | Converts time string with the given pattern to Unix timestamp (in seconds).                                  |\n"
      ],
      "metadata": {
        "id": "uqaBGqXWtyWe"
      },
      "id": "uqaBGqXWtyWe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a DataFrame and Displaying Data\n"
      ],
      "metadata": {
        "id": "Wq0ey9tsLq6j"
      },
      "id": "Wq0ey9tsLq6j"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "            .appName('SparkByExamples.com') \\\n",
        "            .getOrCreate()\n",
        "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
        "df=spark.createDataFrame(data,[\"id\",\"date\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvvtGAnjtanj",
        "outputId": "9d7ee9fe-3eac-4b59-c272-37ad972c235c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|      date|\n",
            "+---+----------+\n",
            "|  1|2020-02-01|\n",
            "|  2|2019-03-01|\n",
            "|  3|2021-03-01|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "id": "tvvtGAnjtanj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###current_date()"
      ],
      "metadata": {
        "id": "zpMi2nHPuAeI"
      },
      "id": "zpMi2nHPuAeI"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(current_date().alias(\"current_date\")  ).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd1Cusjet-pZ",
        "outputId": "463133f2-5ae0-4da3-f989-b479e2287bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|current_date|\n",
            "+------------+\n",
            "|  2023-12-27|\n",
            "+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "id": "Gd1Cusjet-pZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###date_format()"
      ],
      "metadata": {
        "id": "M2aDg3FuuEpj"
      },
      "id": "M2aDg3FuuEpj"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(col(\"date\"),\n",
        "    date_format(col(\"date\"), \"MM-dd-yyyy\").alias(\"date_format\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3iwyLjHuCJx",
        "outputId": "10f7c3eb-462e-4dae-8011-e7842ff73fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|      date|date_format|\n",
            "+----------+-----------+\n",
            "|2020-02-01| 02-01-2020|\n",
            "|2019-03-01| 03-01-2019|\n",
            "|2021-03-01| 03-01-2021|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "id": "M3iwyLjHuCJx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###to_date()"
      ],
      "metadata": {
        "id": "AFujVBM7uIEG"
      },
      "id": "AFujVBM7uIEG"
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(col(\"date\"),\n",
        "    to_date(col(\"date\"), \"yyy-MM-dd\").alias(\"to_date\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQDsQzW0uGbH",
        "outputId": "67dd39ee-c344-443d-bfee-0f6bba35c409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|      date|   to_date|\n",
            "+----------+----------+\n",
            "|2020-02-01|2020-02-01|\n",
            "|2019-03-01|2019-03-01|\n",
            "|2021-03-01|2021-03-01|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "id": "dQDsQzW0uGbH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing MySQL Python Connectors\n"
      ],
      "metadata": {
        "id": "p6cHu3sRL3mc"
      },
      "id": "p6cHu3sRL3mc"
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pymysql\n",
        "!pip install mysql-connector-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0IuBEGAfJ5L",
        "outputId": "8eab4f43-91f5-4630-c7e9-4545099b3f7c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.3.0\n"
          ]
        }
      ],
      "id": "b0IuBEGAfJ5L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing MySQL Connector for Python\n"
      ],
      "metadata": {
        "id": "JGeVAGzNMW-2"
      },
      "id": "JGeVAGzNMW-2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc34d25d"
      },
      "outputs": [],
      "source": [
        "import mysql.connector as sql"
      ],
      "id": "bc34d25d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to a MySQL Database\n"
      ],
      "metadata": {
        "id": "B8EVHCNcMd_j"
      },
      "id": "B8EVHCNcMd_j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1406401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "collapsed": true,
        "outputId": "71ee768e-f4c5-4c95-b07b-d4ca72208d44"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InterfaceError",
          "evalue": "2003: Can't connect to MySQL server on 'localhost:3306' (Errno 111: Connection refused)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mopen_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockaddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7b5abe053b4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ensure you have created a database by name \"company\" by looging into the MySQL instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m db = sql.connect(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'localhost'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/pooling.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCMySQLConnection\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_pure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# Tidy-up underlying socket on failure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/abstracts.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         charset, collation = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 ) from err\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;31m# as the connection is established, set back the read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# do initial handshake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mopen_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    804\u001b[0m             ) from err\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             raise InterfaceError(\n\u001b[0m\u001b[1;32m    807\u001b[0m                 \u001b[0merrno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_strioerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInterfaceError\u001b[0m: 2003: Can't connect to MySQL server on 'localhost:3306' (Errno 111: Connection refused)"
          ]
        }
      ],
      "source": [
        "# Ensure you have created a database by name \"company\" by looging into the MySQL instance\n",
        "\n",
        "db = sql.connect(\n",
        "    host='localhost',\n",
        "    user='root',\n",
        "    password='password',\n",
        "    database='company'\n",
        ")\n"
      ],
      "id": "e1406401"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing SQL Commands with MySQL Connector\n"
      ],
      "metadata": {
        "id": "ASzKC1aCMntP"
      },
      "id": "ASzKC1aCMntP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e01abc15"
      },
      "outputs": [],
      "source": [
        "mycursor = db.cursor()\n",
        "mycursor.execute(\"SELECT VERSION()\")\n",
        "\n",
        "# Drop table if it already exist using execute() method.\n",
        "mycursor.execute(\"DROP TABLE IF EXISTS EMPLOYEE\")\n",
        "\n",
        "# Create table as per the requirement\n",
        "sql = \"\"\"CREATE TABLE EMPLOYEE (\n",
        "         FIRST_NAME  CHAR(20) NOT NULL,\n",
        "         LAST_NAME  CHAR(20),\n",
        "         AGE INT,\n",
        "         SEX CHAR(1),\n",
        "         INCOME FLOAT )\"\"\"\n",
        "\n",
        "mycursor.execute(sql)"
      ],
      "id": "e01abc15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8937cb3d"
      },
      "source": [
        "### INSERT operation on EMPLOYEE table"
      ],
      "id": "8937cb3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f44456be"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"INSERT INTO <EMPLOYEE>\n",
        "         VALUES ('Mac', 'Beth', 20, 'M', 2000)\"\"\"\n",
        "try:\n",
        "   mycursor.execute(sql)\n",
        "   db.commit()\n",
        "except:\n",
        "   db.rollback()"
      ],
      "id": "f44456be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "677a3ca4"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"INSERT INTO EMPLOYEE\n",
        "         VALUES ('Kathy', 'Moss', 20, 'F', 1000)\"\"\"\n",
        "try:\n",
        "   mycursor.execute(sql)\n",
        "   db.commit()\n",
        "except:\n",
        "   db.rollback()"
      ],
      "id": "677a3ca4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "138300a8"
      },
      "source": [
        "### Retrieving all the data from the EMPLOYEE table"
      ],
      "id": "138300a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91e82903",
        "outputId": "44fd4297-455d-421a-e065-294e6aa4fa5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Mac', 'Beth', 20, 'M', 2000.0), ('Kathy', 'Moss', 20, 'F', 1000.0)]\n"
          ]
        }
      ],
      "source": [
        "query = \"SELECT * FROM EMPLOYEE\"\n",
        "mycursor.execute(query)\n",
        "data3 = mycursor.fetchall()\n",
        "print (data3)"
      ],
      "id": "91e82903"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b975f0dd",
        "outputId": "4307950d-7e19-405e-a8b7-15cbc06777c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Mac', 'Beth', 20, 'M', 2000.0)\n",
            "('Kathy', 'Moss', 20, 'F', 1000.0)\n"
          ]
        }
      ],
      "source": [
        "for x in data3:\n",
        "    print(x)"
      ],
      "id": "b975f0dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f628bf2"
      },
      "source": [
        "### Retrieving the data from the 'EMPLOYEE' table based on the condition"
      ],
      "id": "7f628bf2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a291cd37",
        "outputId": "c8c27fbf-ddec-4c44-ca6c-9da0d2214e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Kathy', 'Moss', 20, 'F', 1000.0)]\n"
          ]
        }
      ],
      "source": [
        "# Fetching the rows where salary value is less than 2000\n",
        "sql = \"SELECT * FROM EMPLOYEE \\\n",
        "       WHERE INCOME < '%d'\" % (2000)\n",
        "mycursor.execute(sql)\n",
        "res = mycursor.fetchall()\n",
        "print(res)"
      ],
      "id": "a291cd37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94bee9ca"
      },
      "source": [
        "### Update Operation"
      ],
      "id": "94bee9ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "737553a9"
      },
      "outputs": [],
      "source": [
        "# Prepare SQL query to UPDATE required records\n",
        "sql = \"UPDATE EMPLOYEE SET AGE = AGE + 1  WHERE SEX = '%c'\" % ('M')\n",
        "try:\n",
        "   mycursor.execute(sql)\n",
        "   db.commit()\n",
        "except:\n",
        "   db.rollback()"
      ],
      "id": "737553a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e720c160",
        "outputId": "e7d9a786-cca7-4b7a-83c2-ca9e79a84ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Mac', 'Beth', 21, 'M', 2000.0), ('Kathy', 'Moss', 20, 'F', 1000.0)]\n"
          ]
        }
      ],
      "source": [
        "sql = \"SELECT * FROM EMPLOYEE\"\n",
        "mycursor.execute(sql)\n",
        "upd = mycursor.fetchall()\n",
        "print(upd)"
      ],
      "id": "e720c160"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "406f46ee"
      },
      "source": [
        "### Delete Operation"
      ],
      "id": "406f46ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e9320ae"
      },
      "outputs": [],
      "source": [
        "# Deleting all the rows where age is greater than 20\n",
        "sql = \"DELETE FROM EMPLOYEE WHERE AGE > '%d'\" % (20)\n",
        "try:\n",
        "   mycursor.execute(sql)\n",
        "   db.commit()\n",
        "except:\n",
        "   db.rollback()"
      ],
      "id": "0e9320ae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b886c3a0",
        "outputId": "a3b25686-dc8b-4ccf-ca7e-49ccd4669ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Kathy', 'Moss', 20, 'F', 1000.0)]\n"
          ]
        }
      ],
      "source": [
        "sql = \"SELECT * FROM EMPLOYEE\"\n",
        "mycursor.execute(sql)\n",
        "upd = mycursor.fetchall()\n",
        "print(upd)"
      ],
      "id": "b886c3a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dea041"
      },
      "source": [
        "###Checking all the databases"
      ],
      "id": "a1dea041"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96dc011e",
        "outputId": "2699ae42-571e-46f3-93c7-e23c2615c0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('admissions',)\n",
            "('company',)\n",
            "('hr',)\n",
            "('information_schema',)\n",
            "('mycart',)\n",
            "('myclass',)\n",
            "('mysql',)\n",
            "('performance_schema',)\n",
            "('shop',)\n",
            "('sys',)\n"
          ]
        }
      ],
      "source": [
        "mycursor.execute(\"show databases\")\n",
        "for i in mycursor:\n",
        "    print(i)"
      ],
      "id": "96dc011e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}