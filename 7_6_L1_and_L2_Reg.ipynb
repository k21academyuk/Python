{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6b8dab9-71a1-4b34-bc4f-cdbc28dec1f2",
      "metadata": {
        "id": "f6b8dab9-71a1-4b34-bc4f-cdbc28dec1f2"
      },
      "source": [
        "## What is L1 and L2 Regularization?\n",
        "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. It discourages complex models by reducing the magnitude of the model coefficients.\n",
        "\n",
        "ðŸ”¹ L1 Regularization (Lasso)\n",
        "Name: Least Absolute Shrinkage and Selection Operator (LASSO)\n",
        "\n",
        "Penalty term added:\n",
        "$\n",
        "\\text{Loss}_{\\text{L1}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
        "\n",
        "**Effect:**\n",
        "\n",
        "Adds the absolute value of coefficients as penalty.\n",
        "\n",
        "Encourages sparsity â€” some coefficients become exactly zero.\n",
        "\n",
        "Useful for feature selection.\n",
        "\n",
        "**When to use:**\n",
        "\n",
        "When you suspect many features are irrelevant.\n",
        "\n",
        "When you want to simplify the model by removing features.\n",
        "\n",
        "ðŸ”¹ L2 Regularization (Ridge)\n",
        "Name: Ridge Regression\n",
        "\n",
        "Penalty term added:\n",
        "$\n",
        "\\text{Loss}_{\\text{L2}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
        "$\n",
        "\n",
        "**Effect:**\n",
        "\n",
        "Adds the squared value of coefficients as penalty.\n",
        "\n",
        "Shrinks coefficients closer to zero, but not exactly zero.\n",
        "\n",
        "Prevents large coefficients, improving model generalization.\n",
        "\n",
        "**When to use:**\n",
        "\n",
        "When you have many small/medium effect features.\n",
        "\n",
        "When all features are useful, but you want to reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Importing Libraries\n",
        "In this step, we import the necessary libraries for:\n",
        "- Loading the dataset (`fetch_california_housing`),\n",
        "- Splitting data into training and testing sets (`train_test_split`),\n",
        "- Defining regression models (`Ridge`, `Lasso`, `LinearRegression`),\n",
        "- Evaluating the models (`mean_squared_error`, `r2_score`),\n",
        "- Scaling the features (`StandardScaler`),\n",
        "- And working with data structures (`pandas`).\n"
      ],
      "metadata": {
        "id": "Gfq9F-DhBCQ6"
      },
      "id": "Gfq9F-DhBCQ6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593e8977-4f70-4bc1-a174-45d35875817f",
      "metadata": {
        "id": "593e8977-4f70-4bc1-a174-45d35875817f"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Load and Prepare Data\n",
        "Here, we load the **California Housing Dataset** using `fetch_california_housing`. This dataset contains features about the California housing market.\n",
        "- `X` contains the features (e.g., average rooms, population, etc.),\n",
        "- `y` contains the target values (house prices).\n"
      ],
      "metadata": {
        "id": "lDpVkBkHBIqc"
      },
      "id": "lDpVkBkHBIqc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d81440-4f96-4b0b-b638-2c317b0ed229",
      "metadata": {
        "id": "b1d81440-4f96-4b0b-b638-2c317b0ed229"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Splitting Data into Training and Test Sets\n",
        "In this step, we divide our dataset into training and test sets. We use `train_test_split` to allocate 80% of the data for training and 20% for testing. The `random_state=42` ensures that the split is reproducible each time the code is run.\n"
      ],
      "metadata": {
        "id": "SW4eObD0EKtt"
      },
      "id": "SW4eObD0EKtt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455e6e01-2e27-4a32-91c6-e9ce4c9760c0",
      "metadata": {
        "id": "455e6e01-2e27-4a32-91c6-e9ce4c9760c0"
      },
      "outputs": [],
      "source": [
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling\n",
        "Before applying the machine learning models, we need to scale the features to ensure that all variables have the same scale. We use `StandardScaler` to standardize the features by removing the mean and scaling to unit variance. The training set is fitted and transformed, while the test set is only transformed using the same scaler.\n"
      ],
      "metadata": {
        "id": "42Yzsn4MENx0"
      },
      "id": "42Yzsn4MENx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5815ba6e-af1f-4c79-a5bf-3c3b19bfc4e4",
      "metadata": {
        "id": "5815ba6e-af1f-4c79-a5bf-3c3b19bfc4e4"
      },
      "outputs": [],
      "source": [
        "# Step 3: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Applying L2 Regularization (Ridge Regression)\n",
        "In this step, we apply Ridge regression, which uses L2 regularization. Ridge regression helps in controlling model complexity by adding a penalty term proportional to the square of the coefficients. The `alpha=1.0` parameter controls the strength of the regularization.\n",
        "\n",
        "After fitting the model on the scaled training data, we predict the house prices on the test data and evaluate the model using:\n",
        "- **MSE (Mean Squared Error)**: A measure of prediction error.\n",
        "- **R2 Score**: The proportion of variance explained by the model.\n"
      ],
      "metadata": {
        "id": "mSVDLoG6ERo1"
      },
      "id": "mSVDLoG6ERo1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ef0d80-52c9-4c5d-bf90-da3462f1f6e1",
      "metadata": {
        "id": "d9ef0d80-52c9-4c5d-bf90-da3462f1f6e1",
        "outputId": "94eb0055-53d8-45e9-a432-b1ea870b5742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge Regression\n",
            "MSE: 0.5558548589435988\n",
            "R2 Score: 0.575815742891367\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Apply L2 Regularization (Ridge Regression)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test_scaled)\n",
        "\n",
        "print(\"Ridge Regression\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_ridge))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred_ridge))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying L1 Regularization (Lasso Regression)\n",
        "Now, we apply Lasso regression, which uses L1 regularization. Lasso regression adds a penalty proportional to the absolute value of the coefficients. This encourages sparsity, meaning it may shrink some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "We evaluate the model's performance using MSE and R2 Score just like with Ridge regression.\n"
      ],
      "metadata": {
        "id": "47-McUW1EUn1"
      },
      "id": "47-McUW1EUn1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9f9788-4e6b-4224-bad2-43e0d786a51b",
      "metadata": {
        "id": "4d9f9788-4e6b-4224-bad2-43e0d786a51b",
        "outputId": "defa6819-4414-4a93-ae7c-e234f2d13d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lasso Regression\n",
            "MSE: 0.6796290284328825\n",
            "R2 Score: 0.48136113250290735\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Apply L1 Regularization (Lasso Regression)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso = lasso.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nLasso Regression\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_lasso))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred_lasso))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Applying Linear Regression\n",
        "Finally, we apply standard Linear Regression without any regularization (L1 or L2). Linear Regression models the relationship between the target and the features without adding any penalty term.\n",
        "\n",
        "We evaluate this model's performance using MSE and R2 Score.\n"
      ],
      "metadata": {
        "id": "EEmDRUtEEYt0"
      },
      "id": "EEmDRUtEEYt0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c061314d-a4b8-4417-be14-8ba11384c9fb",
      "metadata": {
        "id": "c061314d-a4b8-4417-be14-8ba11384c9fb",
        "outputId": "fcfa9663-390b-4842-de37-312164758c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lasso Regression\n",
            "MSE: 0.5558915986952442\n",
            "R2 Score: 0.575787706032451\n"
          ]
        }
      ],
      "source": [
        "# Linear Regression\n",
        "LR = LinearRegression()\n",
        "LR.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = LR.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nLasso Regression\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred_lr))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}